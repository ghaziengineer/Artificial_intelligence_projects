{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miaOJPDzEaCS"
      },
      "source": [
        "#IMPORT DEPENDENCIES\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPfNSMYTpHNU"
      },
      "outputs": [],
      "source": [
        "# Import keras packages\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "# Import keras packages\n",
        "import keras\n",
        "from tensorflow.keras import applications \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense , Conv2D , MaxPooling2D , Flatten , GlobalAveragePooling2D , Dropout\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras  #to load model after save\n",
        "from keras.models import load_model\n",
        "# pour calculer la matrice de confusion\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# pour réduisez le taux d'apprentissage lorsqu'une métrique a cessé de s'améliorer.\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# pour pouvoir importer utiliser le modèle pré-entraîner VGG19\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "# pour convertit le vecteur (y) de classe (entiers) en matrice de classe binaire.\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# pour générer des images altere\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# pour initialise le methode de le desente gradien\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Flatten, Dense, Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI6F1IVUEtAg"
      },
      "source": [
        "Connexion to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cseWKt5tEnG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc68ad2d-07c5-4f2c-e803-4727c5116759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbErw3khYE3d"
      },
      "source": [
        "check the distribution of the images in the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFw8TuyYE6jq"
      },
      "outputs": [],
      "source": [
        "def walk_through_dir(directory_name):\n",
        "  for dirpaths,dirnames,filenames in os.walk(directory_name):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpaths}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNHy-goNFCw0"
      },
      "outputs": [],
      "source": [
        "input_data_dir= '/content/drive/MyDrive/drive-download-20220816T231016Z-001' #Dataset URL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifiy classes number"
      ],
      "metadata": {
        "id": "vw8UbbJbAgzp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNxSN27AFK8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84cb2843-bcac-4117-c9bc-e5cf17196724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 4 directories and 0 images in '/content/drive/MyDrive/drive-download-20220816T231016Z-001'\n",
            "There are 0 directories and 939 images in '/content/drive/MyDrive/drive-download-20220816T231016Z-001/Wheat Loose Smut'\n",
            "There are 0 directories and 1030 images in '/content/drive/MyDrive/drive-download-20220816T231016Z-001/Healthy Wheat'\n",
            "There are 0 directories and 696 images in '/content/drive/MyDrive/drive-download-20220816T231016Z-001/Crown and Root Rot'\n",
            "There are 0 directories and 849 images in '/content/drive/MyDrive/drive-download-20220816T231016Z-001/Leaf Rust'\n"
          ]
        }
      ],
      "source": [
        "walk_through_dir(input_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYS5wg4x1S1w"
      },
      "outputs": [],
      "source": [
        "import glob as gb\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad8J-lP-YAV9"
      },
      "outputs": [],
      "source": [
        "def get_dataCategories(input_data_dir):\n",
        "    categories = []\n",
        "    for folder_name in os.listdir(input_data_dir):\n",
        "        if os.path.isdir(os.path.join(input_data_dir, folder_name)):\n",
        "            nbr_files = len(\n",
        "                glob.glob(os.path.join(input_data_dir, folder_name) + \"/**\")\n",
        "            )\n",
        "            categories.append(np.array([folder_name, nbr_files]))\n",
        "\n",
        "    categories.sort(key=lambda a: a[0])\n",
        "    cat = np.array(categories)\n",
        "\n",
        "    return list(cat[:, 0]), list(cat[:, 1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "categories, nbr_files = get_dataCategories(input_data_dir)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\"categorie\": categories, \"numbre of files\": nbr_files})\n",
        "print(\"number of categories: \", len(categories))\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He8w5rXlYwnA"
      },
      "source": [
        "\n",
        "\n",
        "*  Read the images from the dataset \n",
        "*  Resize them using the dimensions img_wid, img_high\n",
        "*  Create the set of features **X** and the labels **y**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2po9AsHYds_",
        "outputId": "0a535e7f-2b3f-486e-88cb-225d2ce387bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (144, 300, 300, 3)\n",
            "y: (144,)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "def create_dataset(input_data_dir, categories, img_wid, img_high):\n",
        "    X, y = [], []\n",
        "    for category in categories:\n",
        "        path = os.path.join(input_data_dir, category)\n",
        "        class_num = categories.index(category)\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path, img))\n",
        "                ima_resize_rgb = cv2.resize(img_array, (img_wid, img_high))\n",
        "\n",
        "                X.append(ima_resize_rgb)\n",
        "                y.append(class_num)\n",
        "\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    y = np.array(y)\n",
        "    X = np.array(X).reshape(y.shape[0], img_wid, img_wid, 3)\n",
        "    return X, y\n",
        "\n",
        "img_wid, img_high = 300, 300\n",
        "X, y = create_dataset(input_data_dir, categories, img_wid, img_high)\n",
        "\n",
        "print(f\"X: {X.shape}\")\n",
        "print(f\"y: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OezSOPzpa_4W"
      },
      "source": [
        "Convert **y** to scaler format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRiUm9WCZVHI"
      },
      "outputs": [],
      "source": [
        "Y = np.reshape(y, (len(y), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWXNNGKwZbRO"
      },
      "source": [
        "Split dataset to train and test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp6oLS0hpdxQ"
      },
      "outputs": [],
      "source": [
        "#il faut l'optimiser \n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoGwBcNxZZI-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y, train_size=0.8\n",
        ")\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"t_train: {y_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxN9Eu2xbLzY"
      },
      "outputs": [],
      "source": [
        "# define training and test sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
        "x_test = X_test\n",
        "\n",
        "# Dimension of the dataset\n",
        "print(f\"x_train:{x_train.shape},  y_train:{y_train.shape}\")\n",
        "print(f\"x_train:{x_val.shape},  y_train:{y_val.shape}\")\n",
        "print(f\"x_train:{x_test.shape},  y_train:{y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC8xLdP4bb6W"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "# One Hot Encoding\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Verifying the dimension after one hot encoding\n",
        "print(f\"x_train:{x_train.shape},  y_train:{y_train.shape}\")\n",
        "print(f\"x_val:{x_val.shape},  y_val:{y_val.shape}\")\n",
        "print(f\"x_test:{x_test.shape},  y_test:{y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrDOrJ7mbk3y"
      },
      "outputs": [],
      "source": [
        "# Image Data Augmentation\n",
        "train_generator = ImageDataGenerator(\n",
        "  rotation_range=30,\n",
        "  zoom_range=0.15,\n",
        "  width_shift_range=0.2,\n",
        "  height_shift_range=0.2,\n",
        "  shear_range=0.15,\n",
        "  horizontal_flip=True,\n",
        "  fill_mode=\"nearest\")\n",
        "\n",
        "val_generator = ImageDataGenerator(\n",
        "    rotation_range=30, horizontal_flip=True, zoom_range=0.15\n",
        ")\n",
        "\n",
        "test_generator = ImageDataGenerator(\n",
        "    rotation_range=30, horizontal_flip=True, zoom_range=0.15\n",
        ")\n",
        "\n",
        "# Fitting the augmentation defined above to the data\n",
        "train_generator.fit(x_train)\n",
        "val_generator.fit(x_val)\n",
        "test_generator.fit(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OVFQ9fjbrmw"
      },
      "source": [
        "#TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yn4pFmbYX4K"
      },
      "outputs": [],
      "source": [
        "number_of_classes =  4\n",
        "batch_size=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oFckBJhBO2l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import load_model, Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "%matplotlib inline\n",
        "from keras.layers import AveragePooling2D \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGEN-VjjA-M1"
      },
      "outputs": [],
      "source": [
        "headmodel = VGG19(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(300, 300, 3)))\n",
        "model = headmodel.output\n",
        "model = AveragePooling2D(pool_size=(5, 5))(model)\n",
        "model = Flatten(name=\"flatten\")(model)\n",
        "model = Dense(512, activation=\"relu\")(model)\n",
        "model = Dropout(0.2)(model)\n",
        "model = Dense(3, activation=\"softmax\")(model)\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "moodel = Model(inputs=headmodel.input, outputs=model)\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the training process\n",
        "for layer in headmodel.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJjsLwDHUdSy"
      },
      "outputs": [],
      "source": [
        "moodel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNTF-C62BeJX",
        "outputId": "bf46be06-d310-4cce-d7d3-4c5e1095774f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "opt = Adam(lr=1e-4)\n",
        "moodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "               metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4gBj7DBBiia"
      },
      "outputs": [],
      "source": [
        "# initialized with actual \"learned\" values versus pure random\n",
        "H = moodel.fit(\n",
        "    train_generator.flow(x_train, y_train, batch_size=16),\n",
        "    steps_per_epoch=len(x_train) // 16,\n",
        "    validation_data=val_generator.flow(x_val,y_val),\n",
        "    validation_steps=len(x_val) // 16,\n",
        "    epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot accuracy & loss"
      ],
      "metadata": {
        "id": "Y1-OCxrcB1jj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkdOinuDJppm"
      },
      "outputs": [],
      "source": [
        "plt.plot(H.history['accuracy'])\n",
        "plt.plot(H.history['val_accuracy'])\n",
        "plt.title('model 4 accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(H.history['loss'])\n",
        "plt.plot(H.history['val_loss'])\n",
        "plt.title('model 4 loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "DcQq-QAFBx-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcc6viD-rCos",
        "outputId": "12fc2f70-c48c-48ae-f81a-510b492ba702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 5s 5s/step - loss: 0.8481 - accuracy: 0.7931\n",
            "Untrained model, accuracy: 79.31%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = moodel.evaluate(x_test, y_test)\n",
        "print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l43DAV4OyhLO"
      },
      "outputs": [],
      "source": [
        "moodel.save('moodelvgg81.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load saved model"
      ],
      "metadata": {
        "id": "QcnzdPAEBBy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "SKySAZ1bBmln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUiT9voMeIbA"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model('/content/moodelvgg81.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrice De Confusion"
      ],
      "metadata": {
        "id": "B9-DZ-lABKDN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpZ8eLBjefuD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Les fonctions utilise affiche la matrice de confusion\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "def cm_plt(ax, cm, classes, cmap, title, normalize):\n",
        "    \"\"\"\n",
        "       affiche la matrice de confusion appartire de l'ax entre\n",
        "\n",
        "    Args:\n",
        "        ax (plt): ax qui sert a affiche la matrice\n",
        "        cm (numpy): matrice de confusion\n",
        "        classes (list): liste des classes\n",
        "        cmap (plt): couluer de la matrice\n",
        "        title (str): titre de la matrice\n",
        "        normalize (booleen): vrai affichre cm normalisee\n",
        "\n",
        "    Returns:\n",
        "        plt: l'ax qui sera affiche\n",
        "    \"\"\"\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(\n",
        "        xticks=np.arange(cm.shape[1]),\n",
        "        yticks=np.arange(cm.shape[0]),\n",
        "        # ... and label them with the respective list entries\n",
        "        xticklabels=classes,\n",
        "        yticklabels=classes,\n",
        "        title=title,\n",
        "        ylabel=\"True label\",\n",
        "        xlabel=\"Predicted label\",\n",
        "    )\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = \".2f\" if normalize else \"d\"\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(\n",
        "                j,\n",
        "                i,\n",
        "                format(cm[i, j], fmt),\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "            )\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Defining function for confusion matrix plot\n",
        "def plt_confusion_mat(cm, classes, fig_size, cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "        afficher la cm normalisee et non normalisee\n",
        "\n",
        "    Args:\n",
        "        cm (numpy): matrice de confusion\n",
        "        classes (list): liste des classes\n",
        "        fig_size (_type_): _description_\n",
        "        cmap (plt, optional): couluer de la matrice. Defaults to plt.cm.Blues.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=fig_size)\n",
        "    ax1 = cm_plt(\n",
        "        ax1,\n",
        "        cm,\n",
        "        classes,\n",
        "        cmap,\n",
        "        title=\"Confusion matrix, without normalization\",\n",
        "        normalize=False,\n",
        "    )\n",
        "\n",
        "    cmn = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "    ax2 = cm_plt(\n",
        "        ax2,\n",
        "        cmn,\n",
        "        classes,\n",
        "        cmap,\n",
        "        title=\"Normalized confusion matrix\",\n",
        "        normalize=True,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_biFyWKdx1P"
      },
      "outputs": [],
      "source": [
        "# Making prediction\n",
        "y_pred = np.argmax(moodel.predict(x_test), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Classification Report"
      ],
      "metadata": {
        "id": "6PsIwLI9BWap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94UqsDdkIAFD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Confusion matrix"
      ],
      "metadata": {
        "id": "bYDz53FkBf0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX2jdHxee704"
      },
      "outputs": [],
      "source": [
        "# get confusion matrix\n",
        "confuision_mat = confusion_matrix(y_true, y_pred)\n",
        "# plot confusion_mat\n",
        "plt_confusion_mat(confuision_mat, classes=categories, fig_size=(20, 7))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "H2QHP_Rx09UR"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}